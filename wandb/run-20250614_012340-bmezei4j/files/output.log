Epoch 001/100 — Recon Loss: 26.9089
Epoch 002/100 — Recon Loss: 26.6877
Epoch 003/100 — Recon Loss: 26.5709
Epoch 004/100 — Recon Loss: 26.2681
Epoch 005/100 — Recon Loss: 23.8879
Epoch 006/100 — Recon Loss: 23.9062
Epoch 007/100 — Recon Loss: 22.5125
Epoch 008/100 — Recon Loss: 22.1100
Epoch 009/100 — Recon Loss: 20.5488
Epoch 010/100 — Recon Loss: 19.2958
Epoch 011/100 — Recon Loss: 17.0820
Epoch 012/100 — Recon Loss: 14.4700
Epoch 013/100 — Recon Loss: 13.4508
Epoch 014/100 — Recon Loss: 12.0730
Epoch 015/100 — Recon Loss: 11.2093
Epoch 016/100 — Recon Loss: 10.7527
Epoch 017/100 — Recon Loss: 10.6184
Epoch 018/100 — Recon Loss: 10.3786
Epoch 019/100 — Recon Loss: 10.3336
Epoch 020/100 — Recon Loss: 10.2704
Epoch 021/100 — Recon Loss: 10.1230
Epoch 022/100 — Recon Loss: 10.3496
Epoch 023/100 — Recon Loss: 10.2596
Epoch 024/100 — Recon Loss: 10.1845
Epoch 025/100 — Recon Loss: 10.1598
Epoch 026/100 — Recon Loss: 10.1376
Epoch 027/100 — Recon Loss: 10.1885
Epoch 028/100 — Recon Loss: 10.1367
Epoch 029/100 — Recon Loss: 10.2142
Epoch 030/100 — Recon Loss: 10.1852
Epoch 031/100 — Recon Loss: 10.2228
Epoch 032/100 — Recon Loss: 10.1765
Epoch 033/100 — Recon Loss: 10.1829
Epoch 034/100 — Recon Loss: 10.0046
Epoch 035/100 — Recon Loss: 10.0940
Epoch 036/100 — Recon Loss: 10.0383
Epoch 037/100 — Recon Loss: 10.1305
Epoch 038/100 — Recon Loss: 9.8973
Epoch 039/100 — Recon Loss: 9.9951
Epoch 040/100 — Recon Loss: 9.5551
Epoch 041/100 — Recon Loss: 9.8559
Epoch 042/100 — Recon Loss: 9.7692
Epoch 043/100 — Recon Loss: 9.4755
Epoch 044/100 — Recon Loss: 9.4813
Epoch 045/100 — Recon Loss: 9.5392
Epoch 046/100 — Recon Loss: 9.2719
Epoch 047/100 — Recon Loss: 8.9246
Epoch 048/100 — Recon Loss: 8.9013
Epoch 049/100 — Recon Loss: 7.9330
Epoch 050/100 — Recon Loss: 8.2268
Epoch 051/100 — Recon Loss: 7.6845
Epoch 052/100 — Recon Loss: 6.2143
Epoch 053/100 — Recon Loss: 6.4487
Epoch 054/100 — Recon Loss: 5.5639
Epoch 055/100 — Recon Loss: 6.3101
Epoch 056/100 — Recon Loss: 6.5759
Epoch 057/100 — Recon Loss: 5.9184
Epoch 058/100 — Recon Loss: 6.5877
Epoch 059/100 — Recon Loss: 6.2353
Epoch 060/100 — Recon Loss: 6.4458
Epoch 061/100 — Recon Loss: 6.9513
Epoch 062/100 — Recon Loss: 6.1380
Epoch 063/100 — Recon Loss: 5.8788
Epoch 064/100 — Recon Loss: 5.3529
Epoch 065/100 — Recon Loss: 7.3028
Epoch 066/100 — Recon Loss: 5.7626
Epoch 067/100 — Recon Loss: 8.3862
Epoch 068/100 — Recon Loss: 7.3823
Epoch 069/100 — Recon Loss: 7.7824
Epoch 070/100 — Recon Loss: 6.9329
Epoch 071/100 — Recon Loss: 7.3808
Epoch 072/100 — Recon Loss: 6.6734
Epoch 073/100 — Recon Loss: 5.9005
Epoch 074/100 — Recon Loss: 4.7535
Epoch 075/100 — Recon Loss: 4.5468
Epoch 076/100 — Recon Loss: 5.2428
Epoch 077/100 — Recon Loss: 6.8164
Epoch 078/100 — Recon Loss: 7.0300
Epoch 079/100 — Recon Loss: 7.4212
Epoch 080/100 — Recon Loss: 8.1615
Epoch 081/100 — Recon Loss: 7.5651
Epoch 082/100 — Recon Loss: 6.7026
Epoch 083/100 — Recon Loss: 5.7804
Epoch 084/100 — Recon Loss: 5.4975
Epoch 085/100 — Recon Loss: 6.8078
Epoch 086/100 — Recon Loss: 7.2995
Epoch 087/100 — Recon Loss: 4.9394
Epoch 088/100 — Recon Loss: 6.7740
Epoch 089/100 — Recon Loss: 5.3652
Epoch 090/100 — Recon Loss: 5.4342
Epoch 091/100 — Recon Loss: 5.1161
Epoch 092/100 — Recon Loss: 5.1704
Epoch 093/100 — Recon Loss: 5.9911
Epoch 094/100 — Recon Loss: 4.2739
Epoch 095/100 — Recon Loss: 5.7965
Epoch 096/100 — Recon Loss: 5.0909
Epoch 097/100 — Recon Loss: 6.2625
Epoch 098/100 — Recon Loss: 4.2759
Epoch 099/100 — Recon Loss: 4.9848
Epoch 100/100 — Recon Loss: 7.0330
Epoch: 1 | Train Loss: 7.0330
Epoch: 1 | Train Loss: 7.0330 | Val Loss: -0.3690 | Val AUROC: 0.5450
Epoch 001/100 — Recon Loss: 6.0505
Epoch 002/100 — Recon Loss: 7.3970
Epoch 003/100 — Recon Loss: 6.6022
Epoch 004/100 — Recon Loss: 6.0267
Epoch 005/100 — Recon Loss: 5.0186
Epoch 006/100 — Recon Loss: 5.8753
Epoch 007/100 — Recon Loss: 5.8137
Epoch 008/100 — Recon Loss: 4.7839
Epoch 009/100 — Recon Loss: 6.3812
Epoch 010/100 — Recon Loss: 5.7165
Epoch 011/100 — Recon Loss: 6.0840
Epoch 012/100 — Recon Loss: 5.3720
Epoch 013/100 — Recon Loss: 4.7470
Epoch 014/100 — Recon Loss: 4.8139
Epoch 015/100 — Recon Loss: 4.5966
Epoch 016/100 — Recon Loss: 6.5415
Epoch 017/100 — Recon Loss: 4.0227
Epoch 018/100 — Recon Loss: 4.2387
Epoch 019/100 — Recon Loss: 5.5798
Epoch 020/100 — Recon Loss: 6.1748
Epoch 021/100 — Recon Loss: 4.4855
Epoch 022/100 — Recon Loss: 5.1689
Epoch 023/100 — Recon Loss: 4.6897
Epoch 024/100 — Recon Loss: 4.4393
Epoch 025/100 — Recon Loss: 3.9574
Epoch 026/100 — Recon Loss: 3.8790
Epoch 027/100 — Recon Loss: 5.1008
Epoch 028/100 — Recon Loss: 5.3771
Epoch 029/100 — Recon Loss: 4.6375
Epoch 030/100 — Recon Loss: 5.7362
Epoch 031/100 — Recon Loss: 3.7886
Epoch 032/100 — Recon Loss: 5.4498
Epoch 033/100 — Recon Loss: 5.3110
Epoch 034/100 — Recon Loss: 4.8619
Epoch 035/100 — Recon Loss: 3.9912
Epoch 036/100 — Recon Loss: 4.3114
Epoch 037/100 — Recon Loss: 4.7779
Epoch 038/100 — Recon Loss: 4.8100
Epoch 039/100 — Recon Loss: 4.7076
Epoch 040/100 — Recon Loss: 5.3782
Epoch 041/100 — Recon Loss: 3.7657
Epoch 042/100 — Recon Loss: 3.8252
Epoch 043/100 — Recon Loss: 5.6752
Epoch 044/100 — Recon Loss: 4.7469
Epoch 045/100 — Recon Loss: 3.9516
Epoch 046/100 — Recon Loss: 4.2143
Epoch 047/100 — Recon Loss: 3.7691
Epoch 048/100 — Recon Loss: 4.5796
Epoch 049/100 — Recon Loss: 4.5318
Epoch 050/100 — Recon Loss: 4.3612
Epoch 051/100 — Recon Loss: 3.6107
Epoch 052/100 — Recon Loss: 4.7162
Epoch 053/100 — Recon Loss: 4.6603
Epoch 054/100 — Recon Loss: 5.6556
Epoch 055/100 — Recon Loss: 5.5486
Epoch 056/100 — Recon Loss: 4.2774
Epoch 057/100 — Recon Loss: 4.0548
Epoch 058/100 — Recon Loss: 5.9450
Epoch 059/100 — Recon Loss: 5.2975
Epoch 060/100 — Recon Loss: 4.3235
Epoch 061/100 — Recon Loss: 4.5387
Epoch 062/100 — Recon Loss: 3.7424
Epoch 063/100 — Recon Loss: 3.7292
Epoch 064/100 — Recon Loss: 4.1806
Epoch 065/100 — Recon Loss: 3.6325
Epoch 066/100 — Recon Loss: 3.5579
Epoch 067/100 — Recon Loss: 4.1570
Epoch 068/100 — Recon Loss: 5.4501
Epoch 069/100 — Recon Loss: 4.9542
Epoch 070/100 — Recon Loss: 4.9317
Epoch 071/100 — Recon Loss: 4.6118
Epoch 072/100 — Recon Loss: 4.8724
Epoch 073/100 — Recon Loss: 4.9711
Epoch 074/100 — Recon Loss: 3.7492
Epoch 075/100 — Recon Loss: 3.4904
Epoch 076/100 — Recon Loss: 4.8246
Epoch 077/100 — Recon Loss: 4.2075
Epoch 078/100 — Recon Loss: 3.9200
Epoch 079/100 — Recon Loss: 4.3706
Epoch 080/100 — Recon Loss: 4.4494
Epoch 081/100 — Recon Loss: 4.4618
Epoch 082/100 — Recon Loss: 5.2129
Epoch 083/100 — Recon Loss: 3.3727
Epoch 084/100 — Recon Loss: 5.1818
Epoch 085/100 — Recon Loss: 6.3703
Epoch 086/100 — Recon Loss: 4.6198
Epoch 087/100 — Recon Loss: 3.9065
Epoch 088/100 — Recon Loss: 4.0935
Epoch 089/100 — Recon Loss: 5.4155
Epoch 090/100 — Recon Loss: 3.5840
Epoch 091/100 — Recon Loss: 3.7576
Epoch 092/100 — Recon Loss: 3.2275
Epoch 093/100 — Recon Loss: 3.2921
Epoch 094/100 — Recon Loss: 4.5054
Epoch 095/100 — Recon Loss: 6.1096
Epoch 096/100 — Recon Loss: 4.6311
Epoch 097/100 — Recon Loss: 3.9798
Epoch 098/100 — Recon Loss: 3.2402
Epoch 099/100 — Recon Loss: 2.8770
Epoch 100/100 — Recon Loss: 3.6602
Epoch: 2 | Train Loss: 3.6602
Epoch: 2 | Train Loss: 3.6602 | Val Loss: -0.3863 | Val AUROC: 0.5649
Traceback (most recent call last):
  File "/Users/qingxuankong/Documents/master_cmu/2025_Summer/Software Design for Data Scientist/BETH_Dataset_Analysis_isla/run_benchmark.py", line 431, in <module>
    main()
  File "/Users/qingxuankong/Documents/master_cmu/2025_Summer/Software Design for Data Scientist/BETH_Dataset_Analysis_isla/run_benchmark.py", line 420, in main
    train(args)
  File "/Users/qingxuankong/Documents/master_cmu/2025_Summer/Software Design for Data Scientist/BETH_Dataset_Analysis_isla/run_benchmark.py", line 163, in train
    train_loss, model = train_gnn(epoch, train_dataset, model)
  File "/Users/qingxuankong/Documents/master_cmu/2025_Summer/Software Design for Data Scientist/BETH_Dataset_Analysis_isla/training.py", line 189, in train_gnn
    train_loss, model = model.fit(dataset.data)
  File "/Users/qingxuankong/Documents/master_cmu/2025_Summer/Software Design for Data Scientist/BETH_Dataset_Analysis_isla/gnn.py", line 64, in fit
    loss = self.model.recon_loss(z, data.edge_index)
  File "/Users/qingxuankong/Documents/master_cmu/2025_Summer/Software Design for Data Scientist/BETH_Dataset_Analysis_isla/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/Users/qingxuankong/Documents/master_cmu/2025_Summer/Software Design for Data Scientist/BETH_Dataset_Analysis_isla/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/Users/qingxuankong/Documents/master_cmu/2025_Summer/Software Design for Data Scientist/BETH_Dataset_Analysis_isla/run_benchmark.py", line 431, in <module>
    main()
  File "/Users/qingxuankong/Documents/master_cmu/2025_Summer/Software Design for Data Scientist/BETH_Dataset_Analysis_isla/run_benchmark.py", line 420, in main
    train(args)
  File "/Users/qingxuankong/Documents/master_cmu/2025_Summer/Software Design for Data Scientist/BETH_Dataset_Analysis_isla/run_benchmark.py", line 163, in train
    train_loss, model = train_gnn(epoch, train_dataset, model)
  File "/Users/qingxuankong/Documents/master_cmu/2025_Summer/Software Design for Data Scientist/BETH_Dataset_Analysis_isla/training.py", line 189, in train_gnn
    train_loss, model = model.fit(dataset.data)
  File "/Users/qingxuankong/Documents/master_cmu/2025_Summer/Software Design for Data Scientist/BETH_Dataset_Analysis_isla/gnn.py", line 64, in fit
    loss = self.model.recon_loss(z, data.edge_index)
  File "/Users/qingxuankong/Documents/master_cmu/2025_Summer/Software Design for Data Scientist/BETH_Dataset_Analysis_isla/venv/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/Users/qingxuankong/Documents/master_cmu/2025_Summer/Software Design for Data Scientist/BETH_Dataset_Analysis_isla/venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
